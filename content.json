{"meta":{"title":"伯利恒之星","subtitle":"人啊，终究是要变态的","description":"HOXI's blog","author":"HOXI","url":"http://hoxii.com"},"pages":[],"posts":[{"title":"机器学习算法(一) KNN算法","slug":"KNN算法","date":"2019-07-23T04:05:30.000Z","updated":"2019-07-23T07:51:20.943Z","comments":true,"path":"2019/07/23/KNN算法/","link":"","permalink":"http://hoxii.com/2019/07/23/KNN算法/","excerpt":"","text":"机器学习算法(一) KNN算法K近邻算法简介：KNN（K Near Neighbor）：K近邻算法，意思就是一个样本的特征可以用与它最近的 K 个邻居的特征来确定，如果 K 个邻居中大多数属于某一类别，那么认为该样本也属于这个类别。例如：有一百个程序员，坐在小秃周围最近的20个人中，有18个人在学机器学习，那么根据 KNN算法 我们认为小秃也在学习机器学习。 KNN算法基本流程： 计算测试样本与各样本间的距离 按照距离的递增关系进行排序 确定 K 值，并选取距离最小的 K 个点 确定这 K 个点所在类别的出现频率 返回频率最高的类别作为测试样本的分类 如何确定 K 值？K：临近数，即要选取多少个邻居用来确定测试样本的类别。K 值的选择非常重要：如果 K 值过小，噪声很有可能会对预测结果产生较大影响，容易发生过拟合；如果 K 值过大，距离较远的邻居也会对预测结果产生影响，在 K==N时，就是取全体实例作为参照，失去了预测的意义。K 值尽量取奇数，以防止类别中的样本数相等，不利于预测。取值方法一般是从 K=1 开始逐渐增大，每次用测试机得到一个分类器误差率，选取误差率最小的 K 值；一般K的取值不超过20，上限是 N 开根号，随着数据集增大，K 的取值范围也要扩大。 如何确定样本间的距离？ 欧氏距离（两点之间的直线距离）各个维度之差的平方相加，开根号。（可以理解为三角形的斜边） 曼哈顿距离（出租车距离）各个维度之差的绝对值之和。（可以理解为三角形两条直角边相加） 切比雪夫距离（国王距离 - 国际象棋）各个维度之差的绝对值中的最大值。 闵可夫斯基距离（闵氏距离）前三种距离的通用表达式： p = 1 时，就是曼哈顿距离； p = 2 时，就是欧氏距离； p = ∞ 时，就是切比雪夫距离 以上四种距离都有明显的缺点： 将各个分量的量纲（单位）同等看待了，也就是说只在意数据特征的取值，而不在意数据特征的意义，很显然身高相差 10cm 与体重相差 10kg 不可以划等号； 未考虑到到各个分量的分布（期望、方差等）可能是不同的。 马氏距离（考虑了特征的量纲、方差的欧氏距离）带有比例的计算欧氏距离 要求：总体样本数大于样本维度。 标准化欧氏距离（加权欧氏距离）针对欧氏距离的缺点进行了改进：既然数据在各维度上分布不同，那么先将各个分量都“标准化”到均值、方差相等，再进行计算。假设样本集 X 的均值为 m，标准差为 s，X 标准化可以表示为： 汉明距离两个等长字符串，将其中一个变为另一个，所需要进行的最小字符替换次数。 例如：11001100 -&gt; 11011101 汉明距离为 2。 汉明重量：字符串相对于等长的零字符串的汉明距离。 杰卡德距离杰卡德相似系数：两个集合的交集元素在他们的并集中所占的比例 杰卡德距离：与杰卡德相似系数相反，两个集合中的不同元素占所有元素的比例 K近邻算法的实现：kd树在KNN预测时，每预测一个点我们就要训练集中每个点到这个点的距离，在数据集很大时，这样做的计算成本非常大。对于 N 个样本 D 个特征的数据集，算法复杂度为O(DN^2)。为了提高 K 近邻算法搜索的效率，可以试着用特殊的结构存储训练数据，从而减少计算距离的次数 - kd树。其原理可以理解为：A 距离 B 很远，B 距离 C 很近，那么 A 距离 C 也很远 那么 kd 树应该如何进行构造？kd 树是一个二叉树，构造 kd 树时首先构造根节点，通过该节点将数据集分为两个部分，此时数据集变成了两个集合和一个节点的组合；再通过递归的方法，不断地对数据集进行切分，直到子区域内没有节点为止；通常，循环选择坐标轴对空间进行切分；选择训练实例点在坐标轴上的中位数作为切分点，这样得到的 kd 树是平衡的（平衡二叉树，左子树和右子树的深度之差的绝对值不超过1，而且各子树也都是平衡二叉树）； 那么在构建 kd 树时，选择向量的哪一维度进行划分呢？选择方差较大的维度，因为方差越大数据越分散，这样构建的树会比较平衡。 例如：样本集{(2, 3), (5, 4), (9, 6), (4, 7), (8, 1), (7, 2)} ​ kd 树的检索在 kd树中，以测试点 A 为圆心，以测试点 A 与 其所在子空间中的样本点 B 之间的距离为半径，画一个圆；如果与其他子空间没有交点，则 测试点 A 与 样本点 B 的分类相同，如果与其他子空间有交点，则计算测试点 A 与有关子空间中的样本点 C、D、F …之间的距离，取距离为最小值的样本点，并确定测试点 A 的分类预期一致。下面是两种情况的对比图： KNN 算法总结KNN算法是最简单有效的分类算法，简单且容易实现。当训练数据集很大时，需要大量的存储空间，而且需要计算待测样本和训练数据集中所有样本的距离，所以会非常耗时； KNN算法对于随机分布的数据集分类效果较差，对于类内间距小，类间间距大的数据集分类效果好，而且对于边界不规则的数据效果好于线性分类器； KNN算法对于样本不均衡的数据效果不好，需要进行改进。改进的方法是对k个近邻数据赋予权重，比如距离测试样本越近，权重越大（马氏距离）。 KNN算法很耗时，时间复杂度为O(n)，一般适用于样本数较少的数据集，当数据量大时，可以将数据以树的形式呈现，能提高速度，常用的有kd-tree和ball-tree。","categories":[{"name":"学习总结","slug":"学习总结","permalink":"http://hoxii.com/categories/学习总结/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://hoxii.com/tags/机器学习/"},{"name":"算法","slug":"算法","permalink":"http://hoxii.com/tags/算法/"},{"name":"KNN","slug":"KNN","permalink":"http://hoxii.com/tags/KNN/"}],"keywords":[{"name":"学习总结","slug":"学习总结","permalink":"http://hoxii.com/categories/学习总结/"}]}]}